{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "In this notebook we will play around with arguibly the most important parameter in Deep Learning, the *learning rate*. The notebook follows the code in the article <a href=\"https://www.geeksforgeeks.org/adjusting-learning-rate-of-a-neural-network-in-pytorch/\">Adjusting Learning Rate of a Neural Netowrk in Pytorch</a> by Herumb Shandilya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, importing modules\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a simple network to recognize hand written digits uring the MNIST dataset. First, we need to load the data, which we have to pre-download and copy to the HPC. An alternative is to download using the login node, as follows.\n",
    "\n",
    "* Open a terminal and SSH into `login1` or `login2`.\n",
    "* Load an appropriate module to use torch, e.g., `hubpy3.7-tf2.3`.\n",
    "\n",
    "```module load hubpy3.7-tf2.3```\n",
    "\n",
    "* Change directory to your data directory.\n",
    "* Start `ipython` and load the dataset using the `dowload=True` flag.\n",
    "* In the Jupyter notebook running on the HPC node, change the data load command appropriately to point to the location of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "train = datasets.MNIST(root='/WAVE/projects/COEN-342-Sp21/data/', train = True, download = False, transform=ToTensor())\n",
    "valid = datasets.MNIST(root='/WAVE/projects/COEN-342-Sp21/data/', train = False, download = False, transform=ToTensor())\n",
    "\n",
    "trainloader = DataLoader(train, batch_size= 32, shuffle=True)\n",
    "validloader = DataLoader(valid, batch_size= 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the model**\n",
    "\n",
    "We are defining a neural network with the following architecture:\n",
    "\n",
    "- Input Layer: 784 nodes, MNIST images are of dimension 28*28 which have 784 pixels so when flatted it’ll become the input to the neural network with 784 input nodes.\n",
    "- Hidden Layer 1: 256 nodes\n",
    "- Hidden Layer 2: 128 nodes\n",
    "- Output Layer: 10 nodes, for 10 classes i.e. numbers 0-9\n",
    "\n",
    "`nn.Linear()` or Linear Layer is used to apply a linear transformation to the incoming data. For those familiar with TensorFlow, it’s pretty much like the Dense Layer. \n",
    "\n",
    "In the forward() method, we start by flattening the image and passing it through each layer, applying the `ReLU` activation before moving on to the next layer. After that, we create our neural network instance, and we're ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING OUR MODEL\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28,64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.out = nn.Linear(32,10)\n",
    "        self.lr = 0.01\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x = x.view(batch_size,-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check to see if a GPU is available and, if it is, send the model to the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU for training.\")\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Criterion and Optimizer**\n",
    "\n",
    "Optimizers define how the weights of the neural network are to be updated, in this tutorial we’ll use the Stochastic Gradient Descent (SGD) optimizer. Optimizers take model parameters and learning rate as the input arguments. There are various optimizers you can try like Adam, Adagrad, etc.\n",
    "\n",
    "The criterion is the loss that you want to minimize which in this case is the CrossEntropyLoss() which is the combination of log_softmax() and NLLLoss(). Different losses are applicable to different types of problems, so be sure to understand the assumptions of the loss and whether they apply to the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING OPTIMIZER, LOSS AND SCHEDULER\n",
    "optimizer = SGD(model.parameters(), lr = 0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Neural Network with Validation**\n",
    "\n",
    "The training step in PyTorch is almost identical almost every time you train it. But before implementing that let’s learn about 2 modes of the model object:\n",
    "\n",
    "- Training Mode:  Set by model.train(), it tells your model that you are training the model. So layers like dropout etc. which behave differently while training and testing can behave accordingly.\n",
    "- Evaluation Mode:  Set by model.eval(), it tells your model that you are testing the model.\n",
    "\n",
    "For each training step, the following operations must occur, in order:\n",
    "\n",
    "- Move data to GPU (Optional)\n",
    "- Clear the gradients using `optimizer.zero_grad()`\n",
    "- Make a forward pass\n",
    "- Calculate the loss\n",
    "- Perform a backward pass using `.backward()` to calculate the gradients\n",
    "- Take optimizer step using `optimizer.step()` to update the weights\n",
    "\n",
    "Finally, we keep track of the lowest validation error we have so far and save the model any time it is better than our best saved model thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e93583688e146f38756e5e8210afbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\t             Training Loss: 12.023097681681316\t             Validation Loss:5.081164587229585\t             LR:0.1\n",
      "Validation Loss Decreased(inf--->1590.404516) \t Saving the model.\n",
      "Epoch 1\t             Training Loss: 4.5322415368715925\t             Validation Loss:3.7228228597642894\t             LR:0.1\n",
      "Validation Loss Decreased(1590.404516--->1165.243555) \t Saving the model.\n",
      "Epoch 2\t             Training Loss: 3.31743111812671\t             Validation Loss:4.258071411769992\t             LR:0.1\n",
      "Epoch 3\t             Training Loss: 2.6708932235181333\t             Validation Loss:2.7514589838088512\t             LR:0.1\n",
      "Validation Loss Decreased(1165.243555--->861.206662) \t Saving the model.\n",
      "Epoch 4\t             Training Loss: 2.2209462110926705\t             Validation Loss:2.9226300595476986\t             LR:0.1\n",
      "Epoch 5\t             Training Loss: 1.9061553725590308\t             Validation Loss:2.691448827074787\t             LR:0.1\n",
      "Validation Loss Decreased(861.206662--->842.423483) \t Saving the model.\n",
      "Epoch 6\t             Training Loss: 1.6725978309859832\t             Validation Loss:2.7434872782137543\t             LR:0.1\n",
      "Epoch 7\t             Training Loss: 1.4371014440650742\t             Validation Loss:3.285088128928012\t             LR:0.1\n",
      "Epoch 8\t             Training Loss: 1.2703619735943774\t             Validation Loss:2.9506348186728957\t             LR:0.1\n",
      "Epoch 9\t             Training Loss: 1.1130373188214997\t             Validation Loss:2.7886654455750324\t             LR:0.1\n",
      "Epoch 10\t             Training Loss: 0.952022093928357\t             Validation Loss:2.7620000764325905\t             LR:0.1\n",
      "Epoch 11\t             Training Loss: 0.8604519038591534\t             Validation Loss:3.015934667023369\t             LR:0.1\n",
      "Epoch 12\t             Training Loss: 0.4125998884215951\t             Validation Loss:2.601155827453467\t             LR:0.010000000000000002\n",
      "Validation Loss Decreased(842.423483--->814.161774) \t Saving the model.\n",
      "Epoch 13\t             Training Loss: 0.3248076724048704\t             Validation Loss:2.576357673380164\t             LR:0.010000000000000002\n",
      "Validation Loss Decreased(814.161774--->806.399952) \t Saving the model.\n",
      "Epoch 14\t             Training Loss: 0.29654754537384337\t             Validation Loss:2.5877289265727463\t             LR:0.010000000000000002\n",
      "Epoch 15\t             Training Loss: 0.2786337523230662\t             Validation Loss:2.576724919828976\t             LR:0.010000000000000002\n",
      "Epoch 16\t             Training Loss: 0.26601247302765646\t             Validation Loss:2.5995719022621153\t             LR:0.010000000000000002\n",
      "Epoch 17\t             Training Loss: 0.2534908076385657\t             Validation Loss:2.632295971297058\t             LR:0.010000000000000002\n",
      "Epoch 18\t             Training Loss: 0.2438638628754144\t             Validation Loss:2.6274891329576637\t             LR:0.010000000000000002\n",
      "Epoch 19\t             Training Loss: 0.23497238810236257\t             Validation Loss:2.658730802479566\t             LR:0.010000000000000002\n",
      "Epoch 20\t             Training Loss: 0.2181422964953507\t             Validation Loss:2.6471409274662396\t             LR:0.0010000000000000002\n",
      "Epoch 21\t             Training Loss: 0.2158119365044559\t             Validation Loss:2.6473086187413286\t             LR:0.0010000000000000002\n",
      "Epoch 22\t             Training Loss: 0.21466903482753164\t             Validation Loss:2.6481212481157854\t             LR:0.0010000000000000002\n",
      "Epoch 23\t             Training Loss: 0.21378112915183106\t             Validation Loss:2.6494119344175027\t             LR:0.0010000000000000002\n",
      "Epoch 24\t             Training Loss: 0.21297287262864412\t             Validation Loss:2.6507496700399726\t             LR:0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE NEURAL NETWORK\n",
    "nepochs = 25\n",
    "min_valid_loss = np.inf\n",
    "for e in trange(nepochs):\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for data, label in trainloader:\n",
    "        # Transfer Data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        target = model(data)\n",
    "        # Find the Loss\n",
    "        train_step_loss = loss(target, label)\n",
    "        # Calculate gradients \n",
    "        train_step_loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        # Calculate Loss\n",
    "        train_loss += train_step_loss.item() * data.size(0)\n",
    "\n",
    "    # Set model to inference/evaluation mode\n",
    "    model.eval()\n",
    "    for data, label in validloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "\n",
    "        target = model(data)\n",
    "        valid_step_loss = loss(target, label)\n",
    "\n",
    "        valid_loss += valid_step_loss.item() * data.size(0)\n",
    "    \n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f'Epoch {e}\\t \\\n",
    "            Training Loss: {train_loss/len(trainloader)}\\t \\\n",
    "            Validation Loss:{valid_loss/len(validloader)}\\t \\\n",
    "            LR:{curr_lr}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving the model.')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'mnist.pth')\n",
    "    scheduler.step(valid_loss/len(validloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-py39] *",
   "language": "python",
   "name": "conda-env-anaconda3-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
